# Домашнее задание к занятию 17 «Инцидент-менеджмент»

## Основная часть

Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

Информация о сбое: 

* [в виде краткой выжимки на русском языке](https://habr.com/ru/post/427301/);
* [развёрнуто на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).

|	Раздел			|  	Описание		|
|-------------------|-------------------|
| **Краткое описание проблемы** | На прошлой неделе произошел инцидент на GitHub, который привел к снижению качества обслуживания на протяжении 24 часов и 11 минут.  |
| **Предшествующие события** | Выполнялась регулярная работа по замене неисправного 100-гигабитного оптического оборудования привела к потере связи между нашим сетевым хабом на восточном побережье США и нашим основным центром обработки данных на восточном побережье США. Соединение между этими местами было восстановлено через 43 секунды, но это кратковременное отключение вызвало цепочку событий, которая привела к 24 часам и 11 минутам деградации сервиса. |
|**Причина инцидента**|Мы используем MySQL для хранения метаданных GitHub и HA MySQL и Orchestrator для управления топологиями наших кластеров MySQL для автоматического выполнения смены основного сервера. Orchestrator учитывает ряд переменных в процессе и построен на основе Raft для достижения консенсуса. После проведения работ по замене оборудования и во время сетевого разделения Orchestrator, который был активен в нашем основном центре обработки данных, начал процесс отказа от лидерства в соответствии с согласием Raft. Узлы Orchestrator на западном побережье США и узлы Orchestrator в облаке США на восточном побережье США смогли установить кворум и начать переключение кластеров для направления записей на западное побережье США. После восстановления связи наш уровень приложений сразу начал направлять трафик записи на новые основные серверы на сайте на западном побережье.Серверы баз данных в центре обработки данных на восточном побережье США содержали кратковременный период записей, которые не были реплицированы в учреждение на западном побережье США. Поскольку кластеры баз данных в обоих центрах обработки данных теперь содержали записи, которых не было в другом центре обработки данных, мы не могли безопасно перевести основной сервер обратно в центр обработки данных на восточном побережье США. |
|**Воздействие**|Было затронуто несколько внутренних систем, что привело к отображению устаревшей и несогласованной информации. В конечном итоге пользовательские данные не пострадали, однако в настоящее время продолжается ручное согласование нескольких секунд записей в базе данных. В течение большей части инцидента GitHub также не мог предоставлять события webhook или собирать и публиковать сайты GitHub Pages.|
|**Обнарудение**|Внутренние системы мониторинга начали генерировать предупреждения, указывающие на множество сбоев в наших системах. На это время было много инженеров, реагирующих и работающих над обработкой поступающих уведомлений|
|**Реакция**|Дежурная группа решила вручную заблокировать наши инструменты развертывания, чтобы предотвратить введение дополнительных изменений и установила статус "желтый" для сайта. Это действие автоматически перевело ситуацию в активный инцидент и отправило оповещение координатору инцидента. Координатор инцидента присоединился, и через две минуты было принято решение перейти в красный статус. На тот момент было понятно, что проблема затронула несколько кластеров баз данных. Были вызваны дополнительные инженеры из команды баз данных GitHub. Они начали исследование текущего состояния, чтобы определить, какие действия необходимо предпринять для ручной конфигурации базы данных на Восточном побережье США в качестве основной для каждого кластера и перестройки топологии репликации. Это усилие было сложным, потому что на тот момент кластер баз данных на Западном побережье впитал записи из нашего тира приложений практически 40 минут. Кроме того, были несколько секунд записей, которые существовали в кластере на Восточном побережье, которые не были реплицированы на Западное побережье и предотвращали репликацию новых записей обратно на Восточное побережье.|
|**Восстановление**| Инженеры, участвующие в команде реагирования на инцидент, начали разрабатывать план по устранению несоответствий данных и внедрению процедур аварийного переключения для MySQL. План включал восстановление из резервных копий, синхронизацию реплик в обоих центрах, возврат к стабильной топологии обслуживания, а затем возобновление обработки задач в очереди. Мы обновили наш статус, чтобы уведомить пользователей, что мы собираемся выполнить контролируемое переключение внутренней системы хранения данных. |
|**Таймлайн**| **21 октября 2018, 22:52 UTC -** Произошла потеря связи между двумя ЦОД на Восток и Западе. Время без связи 43 сек.<br> **21 октября 2018, 22:52 UTC -** Запуск процесса выбора нового лидера, пересоздание топологии кластера БД на западе. После восстановления подключения Оркетсартор направил трафик на запись на новые серверы на западе. После этого на серверах образовались данные, отсутствующие на репликах и не получилось восстановить первичную топологию подключения.<br> **21 октября 2018, 22:54 UTC -** Генерация системой мониторинга оповещений о многочисленных сбоях.<br> **21 октября 2018, 23:02 UTC** Дежурная группа поддержки определила неверную топологию многочисленных кластеров. Orchestrator выбрал топологию при которой подвключались только серверы из ЦОД на Западе США.<br> **21 октября 2018, 23:07 UTC -** Принято решение вручную заблокировать внутренние инструменты развертывания,чтобы предотвратить внесение изменений.<br>**21 октября 2018, 23:09 UTC** Дежурная группа перевела сайт в желтый статус, это оповестило  координатора.<br> **21 октября 2018, 23:11 UTC** К решению проблемы подключился координатор инцидентов и через две минуты изменил статус решения на красный.<br> **21 октября 2018, 23:13 UTC -** Вызвали дополнительных инженеров поддержки СУБД, для исследования состояния и определения дальнейших шагов для ручного перевода БД на Востоке в основную для каждого кластера и перестройки репликации.<br> **21 октября 2018, 23:19 UTC -** Изучив состояния кластеров , приянли решение  о частичной остановке заданий на запись метаданных. Данное решение поставило целостность данных выше удобства использования сайта и времени восстановления.<br> **22 октября 2018, 00:05 UTC -** Инженеры начали разработку плана по устранению несоответствий данных и реализации процедур аварийного восстановления репликкации данных в MySQL. <br> **22 октября 2018, 00:41 UTC -** Начали процесс бэкапа для всех затронутых сбоем кластеров MySQL. Одновременно инженеры искали метод ускорения передачи и восстановления данных.<br> **22 октября 2018, 06:51 UTC -** Несколько кластеров завершили восстановление из резервных копий в центре обработки данных на Восточном побережье США и начали репликацию новых данных с Западного побережья<br> **22 октября 2018, 07:46 UTC -** GitHub опубликовал сообщение в блоге о инциденте и обещал уведомить о его решении.<br> **22 октября 2018, 11:12 UTC -** Все первичные базы сделали основными в ЦОД на Востоке. Сайт улучшил скорость отклика, теперь записи  находились в одном ЦОД с приложениями. К этому времени десятки реплик для чтения базы данных все еще имели многочасовое отставание .<br> **22 октября 2018, 13:15 UTC -** Сталопонятно, что задержки репликации растут, вместо уменьшения. Ранее во время инцидента мы начали предоставлять дополнительные реплики чтения MySQL в общедоступном облаке Восточного побережья США. Как только они стали доступны, стало проще распределить объем запросов на чтение между большим количеством серверов. Снижение использования всех реплик чтения позволило  снизить разрыв в репликах.<br> **22 октября 2018, 16:24 UTC -** После окончания синхронизации, была вовзращена перовначальная топология, это решило проблемы с задержкой и доступностью. Все это время был статус службы был красным, пока не обрабатались все накопившиеся данные.<br> **22 октяюря 2018, 16:45 UTC -** При восстановлении пришлось балансировать увеличенную нагрузку из-за отставших задач. В очереди находилось более пяти миллионов событий вебхуков и 80 тысяч сборок страниц. При включении обработки этих данных мы обработали около 200 000 полезных нагрузок вебхуков, которые превысили внутренний срок и были отброшены. После обнаружения этого мы приостановили обработку и внесли изменения для временного увеличения этого срока. Чтобы избежать дальнейшего размывания надежности наших статус-обновлений, мы оставались в деградированном состоянии до тех пор, пока не завершили обработку всего отставшего объема данных и удостоверились, что наши услуги ясно вернулись к нормальному уровню производительности.<br> **22 октября 2018, 23:03 UTC -** Все ожидающие сборки webhook и страниц были обработаны, а целостность и правильная работа всех систем подтверждены. Статус сайта изменился на зеленый|
|**Последующие действия**| **Технические инициативы**. <br> По мере продолжения внутреннего процесса анализа послеинцидентного периода, мы ожидаем выявление ещё большего объема работы, который необходимо выполнить.<br> <br> 1. Изменение конфигурации Orchestrator для предотвращения продвижения первичных баз данных за пределы региональных границ. Действия Orchestrator вели себя согласно настройкам, несмотря на то, что наш прикладной уровень не мог поддерживать этот измененный типологический переход. Выбор лидера в пределах региона, как правило, безопасен, но внезапное введение межконтинентальной задержки было основным фактором во время этого инцидента. Это было внезапным поведением системы, поскольку ранее мы не сталкивались с таким разделением внутренней сети такого масштаба.<br><br> 2. Мы ускорили миграцию к новому механизму отчетности о состоянии, который обеспечит более богатую площадку для обсуждения активных инцидентов четче и яснее. Во время инцидента многие части GitHub были доступны, но мы могли устанавливать только статусы "зеленый", "желтый" и "красный". Мы осознаем, что это не дает вам точного представления о том, что работает и что нет, и в будущем будем отображать различные компоненты платформы, чтобы вы знали статус каждой услуги.<br><br> 3. Несколько недель перед этим инцидентом мы начали инициативу по инженерному обслуживанию на уровне всей компании для поддержки обработки трафика GitHub из нескольких центров данных в активном/активном/активном дизайне. Этот проект имеет цель поддерживать избыточность N+1 на уровне оборудования. Цель этой работы - выдерживать полный сбой одного центра обработки данных без влияния на пользователей. Это серьезные усилия и потребует времени, но мы считаем, что несколько хорошо связанных сайтов в географическом плане предоставляют набор хороших компромиссов. Этот инцидент добавил срочность к инициативе.<br><br> 4. Мы будем занимать более проактивную позицию в тестировании наших предположений. GitHub - быстрорастущая компания и за последнее десятилетие она накопила свою долю сложности. По мере нашего роста становится все сложнее захватить и передать исторический контекст компромиссов и решений новым поколениям сотрудников.<br><br>**Организационные инициативы** <br> Этот инцидент привел к изменению нашего подхода к надежности сайта. Мы узнали, что более строгие операционные контроли или улучшенные времена реакции недостаточны для обеспечения надежности сайта в системе такой сложности, как наша. Для поддержки этих усилий мы также начнем системную практику проверки сценариев отказа до того, как они могут повлиять на вас. Эта работа включит в себя будущие инвестиции в инструментарий внедрения неисправностей и хаос-инженерии в GitHub.|


---


