# Домашнее задание к занятию «Хранение в K8s. Часть 2»

### Цель задания

В тестовой среде Kubernetes нужно создать PV и продемострировать запись и хранение файлов.

------

### Задание 1

**Что нужно сделать**

Создать Deployment приложения, использующего локальный PV, созданный вручную.

1. Создать Deployment приложения, состоящего из контейнеров busybox и multitool.

Создан [pvc-2.2-1-deployment.yaml](./files/pvc-2.2-1-deployment.yaml)

2. Создать PV и PVC для подключения папки на локальной ноде, которая будет использована в поде.

Создан и применен [pvc-2.2-1.yaml](./files/pvc-2.2-1.yaml) для создания PV и PVC.

```
root@sdpc-lab:~# microk8s kubectl -n netology get pvc
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
local-pvc   Bound    local-pv   1Gi        RWO                           6m11s

root@sdpc-lab:~# microk8s kubectl -n netology get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                STORAGECLASS   REASON   AGE
local-pv   1Gi        RWO            Retain           Bound    netology/local-pvc                           7s

```
3. Продемонстрировать, что multitool может читать файл, в который busybox пишет каждые пять секунд в общей директории. 

Файл доступен и читается из обоих контейнеров и хоста.
```
root@sdpc-lab:~# microk8s kubectl -n netology  exec -it pvc-test-77d458f7db-w5jgp -c multitool -- tail -4 /data/data.txt
Wed Mar 13 06:37:27 UTC 2024
Wed Mar 13 06:37:32 UTC 2024
Wed Mar 13 06:37:37 UTC 2024
Wed Mar 13 06:37:42 UTC 2024

root@sdpc-lab:~# microk8s kubectl -n netology  exec -it pvc-test-77d458f7db-w5jgp -c busybox -- tail -4 /data/data.txt
Wed Mar 13 06:37:32 UTC 2024
Wed Mar 13 06:37:37 UTC 2024
Wed Mar 13 06:37:42 UTC 2024
Wed Mar 13 06:37:47 UTC 2024

root@sdpc-lab:~# tail -5 /data/data.txt
Wed Mar 13 06:37:32 UTC 2024
Wed Mar 13 06:37:37 UTC 2024
Wed Mar 13 06:37:42 UTC 2024
Wed Mar 13 06:37:47 UTC 2024
Wed Mar 13 06:37:52 UTC 2024
```
4. Удалить Deployment и PVC. Продемонстрировать, что после этого произошло с PV. Пояснить, почему.

При удалении Deployment, PVC и PV остаются, потому что они могут использоваться другими ресурсами.

После удаления Deployment,PVC и PV надо удалить самому, если они больше не нужны.

После удаления PVC, PV local-pv останется в состоянии "Available", но не будет связан с PVC. PV сохраняется по нескольким причинам:
- PV может быть использован другими PVC.
- PV может содержать важные данные, которые необходимо сохранить, даже если PVC больше не используется. Автоматическое удаление PV может привести к потере данных.
```
root@sdpc-lab:~# microk8s kubectl -n netology delete deployments.apps/pvc-test
deployment.apps "pvc-test" deleted

root@sdpc-lab:~# microk8s kubectl -n netology get all
NAME                            READY   STATUS        RESTARTS   AGE
pod/pvc-test-77d458f7db-w5jgp   2/2     Terminating   0          6m6s

root@sdpc-lab:~# microk8s kubectl -n netology get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                STORAGECLASS   REASON   AGE
local-pv   1Gi        RWO            Retain           Bound    netology/local-pvc                           25m

root@sdpc-lab:~# microk8s kubectl -n netology get pvc
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
local-pvc   Bound    local-pv   1Gi        RWO                           31m
```
```
root@sdpc-lab:~# microk8s kubectl -n netology delete pvc/local-pvc
persistentvolumeclaim "local-pvc" deleted

root@sdpc-lab:~# microk8s kubectl -n netology get pvc
No resources found in netology namespace.

root@sdpc-lab:~# microk8s kubectl -n netology get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                STORAGECLASS   REASON   AGE
local-pv   1Gi        RWO            Retain           Released   netology/local-pvc                           27m

```
5. Продемонстрировать, что файл сохранился на локальном диске ноды. Удалить PV.  Продемонстрировать что произошло с файлом после удаления PV. Пояснить, почему.

После удаления PV, файл сохранится, потому что был записан в примонтированную директорию внутри контейнеров, все сохранится на локальной файловой системе хоста, т.к. использовался HostPath. k8s не удаляет данные на уровне хоста после удаления PV, данные в этой директории могут использоваться другими приложениями.

```
root@sdpc-lab:~# ls -lah /data/
total 12K
drwxr-xr-x  2 root root 4,0K мар 13 09:33 .
drwxr-xr-x 21 root root 4,0K мар 13 09:07 ..
-rw-r--r--  1 root root 2,3K мар 13 09:40 data.txt

root@sdpc-lab:~# microk8s kubectl -n netology delete pv/local-pv
Warning: deleting cluster-scoped resources, not scoped to the provided namespace
persistentvolume "local-pv" deleted

root@sdpc-lab:~# ls -lah /data/
total 12K
drwxr-xr-x  2 root root 4,0K мар 13 09:33 .
drwxr-xr-x 21 root root 4,0K мар 13 09:07 ..
-rw-r--r--  1 root root 2,3K мар 13 09:40 data.txt
```
5. Предоставить манифесты, а также скриншоты или вывод необходимых команд.

Сделано в п.1-4

------

### Задание 2

**Что нужно сделать**

Создать Deployment приложения, которое может хранить файлы на NFS с динамическим созданием PV.

------

1. Включить и настроить NFS-сервер на MicroK8S.
```
root@sdpc-lab:~# microk8s enable nfs
Infer repository community for addon nfs
Infer repository core for addon helm3
Addon core/helm3 is already enabled
Installing NFS Server Provisioner - Helm Chart 1.4.0

Node Name not defined. NFS Server Provisioner will be deployed on random Microk8s Node.

If you want to use a dedicated (large disk space) Node as NFS Server, disable the Addon and start over: microk8s enable nfs -n NODE_NAME
Lookup Microk8s Node name as: kubectl get node -o yaml | grep 'kubernetes.io/hostname'

Preparing PV for NFS Server Provisioner

persistentvolume/data-nfs-server-provisioner-0 created
"nfs-ganesha-server-and-external-provisioner" has been added to your repositories
Release "nfs-server-provisioner" does not exist. Installing it now.
NAME: nfs-server-provisioner
LAST DEPLOYED: Wed Mar 13 14:46:30 2024
NAMESPACE: nfs-server-provisioner
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The NFS Provisioner service has now been installed.

A storage class named 'nfs' has now been created
and is available to provision dynamic volumes.

You can use this storageclass by creating a `PersistentVolumeClaim` with the
correct storageClassName attribute. For example:

    ---
    kind: PersistentVolumeClaim
    apiVersion: v1
    metadata:
      name: test-dynamic-volume-claim
    spec:
      storageClassName: "nfs"
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 100Mi

NFS Server Provisioner is installed

WARNING: Install "nfs-common" package on all MicroK8S nodes to allow Pods with NFS mounts to start: sudo apt update && sudo apt install -y nfs-common
WARNING: NFS Server Provisioner servers by default hostPath storage from a single Node.

```
2. Создать Deployment приложения состоящего из multitool, и подключить к нему PV, созданный автоматически на сервере NFS.

Созданы и применены файлы 
[nfs-pvc-2.2-2.yaml](./files/nfs-pvc-2.2-2.yaml) и [nfs-pvc-2.2-2-deployment.yaml](./files/nfs-pvc-2.2-2-deployment.yaml)

```
root@sdpc-lab:~# microk8s kubectl -n netology apply -f nfs-pvc-2.2-2-deployment.yaml
deployment.apps/nfs-app created

root@sdpc-lab:~# microk8s kubectl -n netology get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/nfs-app-6bf5ccbc59-7tkkg   1/1     Running   0          15s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nfs-app   1/1     1            1           15s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/nfs-app-6bf5ccbc59   1         1         1       15s

root@sdpc-lab:~# microk8s kubectl -n netology get sc
NAME   PROVISIONER                            RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs    cluster.local/nfs-server-provisioner   Delete          Immediate           true                   7m16s

root@sdpc-lab:~# microk8s kubectl -n netology get  pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                  STORAGECLASS   REASON   AGE
data-nfs-server-provisioner-0              1Gi        RWO            Retain           Bound    nfs-server-provisioner/data-nfs-server-provisioner-0                           7m23s
pvc-46d91015-7e97-4c6a-8b17-a3373314033c   1Gi        RWX            Delete           Bound    netology/nfs-pvc                                       nfs                     3m21s

root@sdpc-lab:~# microk8s kubectl -n netology get  pvc
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-pvc   Bound    pvc-46d91015-7e97-4c6a-8b17-a3373314033c   1Gi        RWX            nfs            3m24s

```
3. Продемонстрировать возможность чтения и записи файла изнутри пода. 
```
root@sdpc-lab:~# microk8s kubectl -n netology exec -it nfs-app-6bf5ccbc59-7tkkg -c multitool -- /bin/bash

bash-5.1# mount|grep nfs
10.152.183.27:/export/pvc-46d91015-7e97-4c6a-8b17-a3373314033c on /data type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.152.183.27,mountvers=3,mountport=20048,mountproto=udp,local_lock=none,addr=10.152.183.27)

bash-5.1# ls -lah /data/
total 8K
drwxrwsrwx    2 root     root        4.0K Mar 13 12:12 .
drwxr-xr-x    1 root     root        4.0K Mar 13 12:15 ..

bash-5.1# echo "Test NFS write" > /data/test.txt

bash-5.1# cat /data/test.txt
Test NFS write
```
4. Предоставить манифесты, а также скриншоты или вывод необходимых команд.

Сделано в п.1-3

------

