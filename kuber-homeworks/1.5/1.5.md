# Домашнее задание к занятию «Сетевое взаимодействие в K8S. Часть 2»

### Цель задания

В тестовой среде Kubernetes необходимо обеспечить доступ к двум приложениям снаружи кластера по разным путям.

------

### Задание 1. Создать Deployment приложений backend и frontend

1. Создать Deployment приложения _frontend_ из образа nginx с количеством реплик 3 шт.
2. Создать Deployment приложения _backend_ из образа multitool. 
3. Добавить Service, которые обеспечат доступ к обоим приложениям внутри кластера. 
4. Продемонстрировать, что приложения видят друг друга с помощью Service.
5. Предоставить манифесты Deployment и Service в решении, а также скриншоты или вывод команды п.4.

	[backend-1.5-1-deployment.yaml](./files/backend-1.5-1-deployment.yaml)
	
	[frontend-1.5-1-deployment.yaml](./files/frontend-1.5-1-deployment.yaml)
	
	[services-1.5-1.yaml](./files/services-1.5-1.yaml)

***Созданные pod и service.***
```
root@sdpc-lab:~# microk8s kubectl -n netology get pods
NAME                                   READY   STATUS    RESTARTS   AGE
multitool-pod                          1/1     Running   0          14h
frontend-deployment-6d4fb6b95f-dcsvs   1/1     Running   0          89s
frontend-deployment-6d4fb6b95f-mlxz7   1/1     Running   0          89s
frontend-deployment-6d4fb6b95f-zn5xd   1/1     Running   0          89s
backend-deployment-59876cc499-gxdw8    1/1     Running   0          79s

root@sdpc-lab:~# microk8s kubectl -n netology get svc
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
backend-service    ClusterIP   10.152.183.98   <none>        8080/TCP   2s
frontend-service   ClusterIP   10.152.183.44   <none>        80/TCP     2s
```
***Проверка доступности backend сервиса из frontend-deployment pod.***
```
root@sdpc-lab:~# microk8s kubectl -n netology  exec -it frontend-deployment-6d4fb6b95f-dcsvs -- curl -s -o /dev/null -v backend-service.netology.svc.cluster.local:8080
*   Trying 10.152.183.98:8080...
* Connected to backend-service.netology.svc.cluster.local (10.152.183.98) port 8080 (#0)
> GET / HTTP/1.1
> Host: backend-service.netology.svc.cluster.local:8080
> User-Agent: curl/7.88.1
> Accept: */*
>
< HTTP/1.1 200 OK
< Server: nginx/1.18.0
< Date: Mon, 11 Mar 2024 09:35:34 GMT
< Content-Type: text/html
< Content-Length: 1589
< Last-Modified: Mon, 11 Mar 2024 09:28:19 GMT
< Connection: keep-alive
< ETag: "65eeceb3-635"
< Accept-Ranges: bytes
<
{ [1589 bytes data]
* Connection #0 to host backend-service.netology.svc.cluster.local left intact
```
***Проверка доступности frontend сервиса из backend-deployment pod.***
```
root@sdpc-lab:~# microk8s kubectl -n netology  exec -it backend-deployment-59876cc499-gxdw8 -- curl -s -o /dev/null -v frontend-service.netology.svc.cluster.local:80
*   Trying 10.152.183.44:80...
* Connected to frontend-service.netology.svc.cluster.local (10.152.183.44) port 80 (#0)
> GET / HTTP/1.1
> Host: frontend-service.netology.svc.cluster.local
> User-Agent: curl/7.79.1
> Accept: */*
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx/1.25.4
< Date: Mon, 11 Mar 2024 09:37:05 GMT
< Content-Type: text/html
< Content-Length: 615
< Last-Modified: Wed, 14 Feb 2024 16:03:00 GMT
< Connection: keep-alive
< ETag: "65cce434-267"
< Accept-Ranges: bytes
<
{ [615 bytes data]
* Connection #0 to host frontend-service.netology.svc.cluster.local left intact

```


------

### Задание 2. Создать Ingress и обеспечить доступ к приложениям снаружи кластера

1. Включить Ingress-controller в MicroK8S.
```
root@sdpc-lab:~# microk8s enable ingress
Infer repository core for addon ingress
Enabling Ingress
ingressclass.networking.k8s.io/public created
ingressclass.networking.k8s.io/nginx created
namespace/ingress created
serviceaccount/nginx-ingress-microk8s-serviceaccount created
clusterrole.rbac.authorization.k8s.io/nginx-ingress-microk8s-clusterrole created
role.rbac.authorization.k8s.io/nginx-ingress-microk8s-role created
clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-microk8s created
rolebinding.rbac.authorization.k8s.io/nginx-ingress-microk8s created
configmap/nginx-load-balancer-microk8s-conf created
configmap/nginx-ingress-tcp-microk8s-conf created
configmap/nginx-ingress-udp-microk8s-conf created
daemonset.apps/nginx-ingress-microk8s-controller created
Ingress is enabled

```
2. Создать Ingress, обеспечивающий доступ снаружи по IP-адресу кластера MicroK8S так, чтобы при запросе только по адресу открывался _frontend_ а при добавлении /api - _backend_.
```
root@sdpc-lab:~# microk8s kubectl -n netology apply -f ingress-1.5-1.yaml
ingress.networking.k8s.io/app-ingress created

root@sdpc-lab:~# microk8s kubectl -n netology get ing
NAME          CLASS    HOSTS            ADDRESS   PORTS   AGE
app-ingress   public   sdpc-lab.local             80      8s

root@sdpc-lab:~# microk8s kubectl -n netology describe ingress/app-ingress
Name:             app-ingress
Labels:           <none>
Namespace:        netology
Address:          127.0.0.1
Ingress Class:    public
Default backend:  <default>
Rules:
  Host            Path  Backends
  ----            ----  --------
  sdpc-lab.local
                  /      frontend-service:80 (10.1.92.137:80,10.1.92.138:80,10.1.92.139:80)
                  /api   backend-service:8080 (10.1.92.140:8080)
Annotations:      nginx.ingress.kubernetes.io/rewrite-target: /
Events:
  Type    Reason  Age                From                      Message
  ----    ------  ----               ----                      -------
  Normal  Sync    92s (x3 over 24m)  nginx-ingress-controller  Scheduled for sync

```
3. Продемонстрировать доступ с помощью браузера или `curl` с локального компьютера.

Hostname из ingress добавил в hosts
```
/drives/c/kube:~ cat /etc/hosts
----cut----
10.99.16.216 sdpc-lab.local
```
***Проверка sdpc-lab.local/***
```
/drives/c/kube:~ curl sdpc-lab.local/
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```
***Проверяем sdpc-lab.local/api***
```                                                                                     /drives/c/kube:~ curl sdpc-lab.local/api
Praqma Network MultiTool (with NGINX) - backend-deployment-59876cc499-gxdw8 - 10.1.92.140 - HTTP: 8080 , HTTPS: 8443
<br>
<hr>
<br>

<h1>05 Jan 2022 - Press-release: `Praqma/Network-Multitool` is now `wbitt/Network-Multitool`</h1>

<h2>Important note about name/org change:</h2>
<p>
Few years ago, I created this tool with Henrik Høegh, as `praqma/network-multitool`. Praqma was bought by another company, and now the "Praqma" brand is being dismantled. This means the network-multitool's git and docker repositories must go. Since, I was the one maintaining the docker image for all these years, it was decided by the current representatives of the company to hand it over to me so I can continue maintaining it. So, apart from a small change in the repository name, nothing has changed.<br>
</p>
<p>
The existing/old/previous container image `praqma/network-multitool` will continue to work and will remain available for **"some time"** - may be for a couple of months - not sure though.
</p>
<p>
- Kamran Azeem <kamranazeem@gmail.com> <a href=https://github.com/KamranAzeem>https://github.com/KamranAzeem</a>
</p>

<h2>Some important URLs:</h2>

<ul>
  <li>The new official github repository for this tool is: <a href=https://github.com/wbitt/Network-MultiTool>https://github.com/wbitt/Network-MultiTool</a></li>

  <li>The docker repository to pull this image is now: <a href=https://hub.docker.com/r/wbitt/network-multitool>https://hub.docker.com/r/wbitt/network-multitool</a></li>
</ul>

<br>
Or:
<br>

<pre>
  <code>
  docker pull wbitt/network-multitool
  </code>
</pre>


<hr>

```
4. Предоставить манифесты и скриншоты или вывод команды п.2.

	[ingress-1.5-2.yaml](./files/ingress-1.5-2.yaml)

Вывод команд проверки работы inress дан в п.3. Не стал копировать сюда одно и тоже.

------
